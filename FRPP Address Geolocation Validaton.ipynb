{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5c25f2f154e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import requests as re\n",
    "import json\n",
    "import pyodbc\n",
    "import urllib3\n",
    "import os\n",
    "import glob\n",
    "from geopy import distance\n",
    "from utils import config\n",
    "from utils import db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://agsivvwa.gsa.gov/gsagis1/rest/services/base/GeoLocate/GeocodeServer/geocodeAddresses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverName = config.serverName\n",
    "password = config.password\n",
    "database= config.database\n",
    "userName =  config.userName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"SELECT [ReportingAgency__c], [ReportingBureau__c], [RealPropertyUniqueId__c], [StateName__c] as Region, [CityName__c] as City, CAST([ZipCode__c] as VARCHAR(5)) as Postal, [StreetAddress__c] as Address FROM [OGPD2D].[dbo].[RP_FRPP_Salesforce_daily] where StreetAddress__c is not null and CountryName__c = 'United States' and DATEADD(SECOND, CAST(LastModifiedDate as BIGINT)/1000 ,'1970/1/1') > '2020/2/12'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frpp():\n",
    "    '''\n",
    "    Sends credentials and SQL query, returns to dataframe\n",
    "    '''\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+serverName+';DATABASE='+database+';UID='+userName+';PWD='+ password+'')\n",
    "    df = pd.read_sql(sql_query, cnxn)\n",
    "    cnxn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRPP_df = get_frpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a unique ID as identifiers aren't standardized accross agencies. \n",
    "FRPP_df['OBJECTID'] = FRPP_df[['ReportingAgency__c','ReportingBureau__c','RealPropertyUniqueId__c']].apply(lambda x: '_'.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All this does is disable the warning for sending an unsecure request to an https site. The warning is printed with each\n",
    "### request and consumed a lot of memory.\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json():\n",
    "    '''\n",
    "    Loops through 1000 rows of FRPP_df and creates JSON for each row. Sends get request to API,\n",
    "    captures JSON response and normalizes to dataframe and appends it to json_arr. Saving as a \n",
    "    list of objects vs dataframe for performance.\n",
    "    '''\n",
    "    json_arr = []\n",
    "    init = iter_num * 1000\n",
    "    x = init + 1000\n",
    "    for index, row in FRPP_df[init:x].iterrows():\n",
    "        FRPP_address = json.dumps(\n",
    "            {       \n",
    "                \"records\": [\n",
    "                    {\n",
    "                        \"attributes\": {\n",
    "                        \"OBJECTID\": row['OBJECTID'],\n",
    "                        \"Address\": row['Address'],\n",
    "                        \"City\": row['City'],\n",
    "                        \"Region\": row['Region'],\n",
    "                        \"Postal\": row['Postal']\n",
    "                    }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            )\n",
    "\n",
    "\n",
    "        r = re.get(url, params = { 'addresses': FRPP_address, 'f':'pjson'},verify = False)\n",
    "        temp_df = json_normalize(r.json()['locations'])\n",
    "        temp_df.insert(0,'OBJECTID',row['OBJECTID'])\n",
    "        json_arr.append(temp_df)\n",
    "\n",
    "    return json_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_excel():\n",
    "    '''\n",
    "    Convert json_arr to dataframe. Write dataframe to excel.\n",
    "    '''\n",
    "    geocoded_df = pd.concat(json_arr)\n",
    "    file_name=  'data/archive/geocoded' + str(iter_num) + '.xlsx'\n",
    "    geocoded_df.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multi_excel(path):\n",
    "    '''\n",
    "    Given a file path with wildcard and extension, parse all files with that extension in directory \n",
    "    into a single dataframe.\n",
    "    '''\n",
    "    \n",
    "    all_files = glob.glob(path)\n",
    "    li = []\n",
    "    \n",
    "    for filename in all_files:\n",
    "        df = pd.read_excel(filename, index_col=None, header=0)\n",
    "        df['Source'] = os.path.basename(filename)\n",
    "        li.append(df)\n",
    "        \n",
    "    df = pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 916):\n",
    "    '''\n",
    "    This was created to be able to stop/start iteration through all records as desired. Each increment\n",
    "    reflects 1000 rows of data, leverages get_json function to get json response from api and json_to_excel\n",
    "    function to convert json to dataframe and write to excel. Each file contains 1000 records. This allowed \n",
    "    me to start at x if something timed/errored out. \n",
    "    '''\n",
    "    iter_num = i\n",
    "    \n",
    "    json_arr = get_json()\n",
    "    \n",
    "    json_to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all output files in folder, convert to single dataframe and write to excel\t\n",
    "final_df = read_multi_excel('data/archive/*.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.rename(columns={\n",
    "    'attributes.AddNum' :  'AddNum',\n",
    "    'attributes.AddNumFrom' :  'AddNumFrom',\n",
    "    'attributes.AddNumTo' :  'AddNumTo',\n",
    "    'attributes.Addr_type' :  'Addr_type',\n",
    "    'attributes.City' :  'City',\n",
    "    'attributes.Country' :  'Country',\n",
    "    'attributes.DisplayX' :  'DisplayX',\n",
    "    'attributes.DisplayY' :  'DisplayY',\n",
    "    'attributes.Distance' :  'Distance',\n",
    "    'attributes.LangCode' :  'LangCode',\n",
    "    'attributes.Loc_name' :  'Loc_name',\n",
    "    'attributes.Match_addr' :  'Match_addr',\n",
    "    'attributes.Postal' :  'Postal',\n",
    "    'attributes.Rank' :  'Rank',\n",
    "    'attributes.Region' :  'Region',\n",
    "    'attributes.RegionAbbr' :  'RegionAbbr',\n",
    "    'attributes.ResultID' :  'ResultID',\n",
    "    'attributes.Score' :  'Score',\n",
    "    'attributes.Side' :  'Side',\n",
    "    'attributes.StAddr' :  'StAddr',\n",
    "    'attributes.StDir' :  'StDir',\n",
    "    'attributes.StName' :  'StName,',\n",
    "    'attributes.StPreDir' :  'StPreDir',\n",
    "    'attributes.StPreType' :  'StPreType',\n",
    "    'attributes.StType' :  'StType',\n",
    "    'attributes.Status' :  'Status',\n",
    "    'attributes.X' :  'X',\n",
    "    'attributes.Xmax' :  'Xmax',\n",
    "    'attributes.Xmin' :  'Xmin',\n",
    "    'attributes.Y' :  'Y',\n",
    "    'attributes.Ymax' :  'Ymax',\n",
    "    'attributes.Ymin' :  'Ymin',\n",
    "    'location.x' :  'location.x',\n",
    "    'location.y ' :  'location.y'}, \n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_excel('data/FRPP_geocoded.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: incorporate feature that compares reported latitude/longitudue point\n",
    "### to the geolocated lat/long as a point distance. This will allow FRPP team to\n",
    "### flag responses that have a variance that is invalid. SQL Query will need to be\n",
    "### changed to include agency reported lat/long. Column names are placeholder.\n",
    "### to do: merge the reported lat/long values from orignal FRPP_df into final_df\n",
    "### with join on OBJECTID to add those new columns\n",
    "### code below will output distance between points in miles in new column\n",
    "### geopy module needs to be added to DSVD\n",
    "\n",
    "geo_lat = 'location.x'\n",
    "geo_long = 'location.x'\n",
    "input_lat = 'SUBMITTED LATITUDE'\n",
    "input_long = 'SUBMITTED LONGITUDE'\n",
    "final_df['DISTANCE VARIANCE'] = final_df.apply(\n",
    "    (lambda row: distance.distance(\n",
    "        (row[geo_lat], row[geo_long]),\n",
    "        (row[input_lat], row[input_long])\n",
    "    ).miles),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "### TO DO: Current results write to excel, optimal output is write to SQL Directly\n",
    "\n",
    "### TO DO: Current inital SQL query reads all records, optimal tool would read\n",
    "### updated values in some time frame (i.e. daily/weekly) as fully automated tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}